{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Benchmarking!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "%uv pip install vllm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "messages=[{\"role\":\"user\",\"content\":\"Say hi in 5 words\"}]\n",
        "max_tokens = 128\n",
        "sp = SamplingParams(temperature=0.3, max_tokens=max_tokens)\n",
        "# llm = LLM(model=MODEL_ID, dtype=\"bfloat16\", tokenizer_mode=\"mistral\", config_format=\"mistral\", load_format=\"mistral\")\n",
        "llm = LLM(model=MODEL_ID, dtype=\"bfloat16\", tokenizer_mode=\"mistral\", config_format=\"mistral\", load_format=\"mistral\", max_model_len=2048)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# prompt = messages[-1][\"content\"] if messages else \"Hello\"\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Just in case\n",
        "\n",
        "# query = \"whoo are you? in 5 words\"\n",
        "messages=[{\"role\": \"system\", \"content\": \"You are concise.\"},{\"role\":\"user\",\"content\":\"Say hi in 5 words\"}]\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "\n",
        "stops = []\n",
        "sp = SamplingParams(temperature=0.3, max_tokens=128, stop=[\"[INST]\"])\n",
        "outputs = llm.generate([prompt], sp)\n",
        "outputs[0].outputs[0].text.lstrip().rstrip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# release vllm mem from gpu\n",
        "import gc, torch\n",
        "# del llm\n",
        "gc.collect(); torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "\n",
        "# Load tokenizer\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
        "# if tok.pad_token is None:\n",
        "#     tok.pad_token = tok.eos_token\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=torch.bfloat16,   # or \"auto\" if your GPU supports it\n",
        "    device_map=\"auto\",            # put it on the available GPU\n",
        ").eval()\n",
        "\n",
        "# Example chat messages\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are concise.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Say hi in five words\"}\n",
        "]\n",
        "\n",
        "# Build the prompt string with the same template HF uses\n",
        "prompt = tok.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,   # ensures assistant turn starts\n",
        ")\n",
        "\n",
        "# Encode\n",
        "inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Generate\n",
        "with torch.no_grad():\n",
        "    out = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=128,\n",
        "        temperature=0.3,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tok.eos_token_id,\n",
        "    )\n",
        "\n",
        "# Decode and strip the prompt if echoed\n",
        "# full_text = tok.decode(out[0], skip_special_tokens=True)\n",
        "# completion = full_text[len(prompt):].strip() if full_text.startswith(prompt) else full_text.strip()\n",
        "\n",
        "# print(\"HF baseline output:\", repr(completion))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Hello, there!'"
            ]
          },
          "execution_count": 199,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tok = AutoTokenizer.from_pretrained(MODEL_ID, padding_side=\"left\", truncation_side=\"left\")\n",
        "tok.pad_token = tok.eos_token\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are concise.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Say hi in five words\"}\n",
        "]\n",
        "\n",
        "# Build the prompt string with the same template HF uses\n",
        "input_id = tok.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    padding=True,\n",
        "    return_tensors=\"pt\",\n",
        "    add_generation_prompt=True,   # ensures assistant turn starts\n",
        ").to(model.device)\n",
        "\n",
        "# Encode\n",
        "# inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Generate\n",
        "with torch.no_grad():\n",
        "    out = model.generate(\n",
        "        input_id,\n",
        "        max_new_tokens=128,\n",
        "        pad_token_id=tok.eos_token_id,\n",
        "    )\n",
        "\n",
        "# Decode and strip the prompt if echoed\n",
        "input_len = input_id.shape[1]\n",
        "# out, out.shape, input_id, input_id.shape\n",
        "full_text = tok.decode(out[0,input_len:], skip_special_tokens=True)\n",
        "full_text\n",
        "# len(prompt)print(\"Full text:\", repr(full_text))\n",
        "# print(\"Prompt length:\", len(prompt))\n",
        "# print(\"full_text length: \", len(full_text))\n",
        "# inputs, prompt, full_text\n",
        "# len(full_text)\n",
        "# completion = full_text[len(prompt):].strip() if full_text.startswith(prompt) else full_text.strip()\n",
        "\n",
        "# print(\"HF baseline output:\", repr(completion))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([1, 21]), torch.Size([1, 16]))"
            ]
          },
          "execution_count": 172,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# inputs.input_ids.shape[1]\n",
        "out.shape, input_id.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "tok = AutoTokenizer.from_pretrained(MODEL_ID, padding_side=\"left\", truncation_side=\"left\")\n",
        "tok.pad_token = tok.eos_token\n",
        "\n",
        "queries = [\n",
        "    \"Say hi in five words\",\n",
        "    \"What's the capital of France?\",\n",
        "    \"Tell me a joke\",\n",
        "    \"Explain quantum physics briefly\"\n",
        "]\n",
        "\n",
        "batch = [\n",
        "    [{\"role\": \"system\", \"content\": \"You are concise.\"},\n",
        "    {\"role\": \"user\", \"content\": query}] for query in queries\n",
        "]\n",
        "\n",
        "# Build the prompt string with the same template HF uses\n",
        "input_id = tok.apply_chat_template(\n",
        "    batch,\n",
        "    tokenize=True,\n",
        "    padding=True,\n",
        "    return_tensors=\"pt\",\n",
        "    add_generation_prompt=True,   # ensures assistant turn starts\n",
        ").to(model.device)\n",
        "\n",
        "input_id, input_id.shape\n",
        "\n",
        "# Generate\n",
        "with torch.no_grad():\n",
        "    out = model.generate(\n",
        "        input_id,\n",
        "        max_new_tokens=128,\n",
        "        pad_token_id=tok.eos_token_id,\n",
        "    )\n",
        "\n",
        "# Decode and strip the prompt if echoed\n",
        "input_len = input_id.shape[1]\n",
        "# out, out.shape, input_id, input_id.shape\n",
        "full_text = tok.decode(out[:,input_len:], skip_special_tokens=True)\n",
        "full_text"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
